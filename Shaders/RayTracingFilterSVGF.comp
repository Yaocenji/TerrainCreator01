#version 450 core

// kernel对应屏幕空间的像素
layout (local_size_x = 32, local_size_y = 32) in;

#define PI 3.14159265358979323846

/**
    uniform内容
*/

// 屏幕(RT)大小
uniform int screenWidth;
uniform int screenHeight;

// 渲染源数据
// 为什么使用texture作为源数据而非image2D？
// 因为预渲染的渲染结果是GBuffer Texture
// 屏幕RT颜色通道
uniform sampler2D RTColor;
// 世界空间坐标通道
uniform sampler2D RTWorldPos;
// 法线通道
uniform sampler2D RTNormal;
// 运动矢量
uniform sampler2D RTMotionVector;
// 深度图
uniform sampler2D RTDepth;

// 摄像机位置
uniform vec3 cameraPos;
// 摄像机near
uniform float near;
// 摄像机far
uniform float far;

// a-trous wavelet滤波层数，影响采样间隔
uniform int filterStep;
// gaussain kernel半径，边长等于半径*2+1
uniform int kernelRadius;

// 渲染源
// 为何使用image2D而非texture
// -- 因为使用了计算着色器而非片元着色器，计算着色器无法写入数据到texture
layout (binding = 0, rgba32f) uniform image2D RTSrc;


layout (binding = 2, rgba32f) uniform image2D RTColorVariance;

// 渲染目标
// 为何使用image2D而非texture
// -- 因为使用了计算着色器而非片元着色器，计算着色器无法写入数据到texture
layout (binding = 1, rgba32f) uniform image2D RTTarget;

struct comparison1D{
    float value0;
    float value1;
};

struct comparison4D{
    vec4 value0;
    vec4 value1;
};

// 将深度缓冲值转化为世界空间下距离
float LinearizeDepth(float depth)
{
    float z = depth * 2.0 - 1.0;
    return (2.0 * near * far) / (far + near - z * (far - near));
}
// 求一处像素得到线性深度值
float LinearDepthValue(ivec2 posIndex){
    return LinearizeDepth(texture(RTDepth, vec2(vec2(posIndex.xy) + vec2(0.5, 0.5)) / vec2(screenWidth, screenHeight)).r);
}
// 求一处像素的线性深度值的梯度
vec2 LinearDepthGradience(ivec2 posIndex){
    float dep00 = LinearDepthValue(posIndex);
    float dep01 = LinearDepthValue(posIndex + ivec2(0, 1));
    float dep10 = LinearDepthValue(posIndex + ivec2(1, 0));
    return vec2(dep10 - dep00, dep01 - dep00);
}
// 求一处像素的线性深度值的梯度，减少重复采样的算法
vec2 LinearDepthGradience(ivec2 posIndex, float thisDepth){
    float dep01 = LinearDepthValue(posIndex + ivec2(0, 1));
    float dep10 = LinearDepthValue(posIndex + ivec2(1, 0));
    return vec2(dep10 - thisDepth, dep01 - thisDepth);
}
// 求一处像素的联合双边滤波深度权重项
float weightDepth(ivec2 thisIndex, ivec2 targetIndex, vec2 thisDepthGrad, float thisDepth){
    float targetDepth = LinearDepthValue(targetIndex);
    // 系数是σD
    float denom = 1.0 * length(vec2(thisIndex) - vec2(targetIndex)) * length(thisDepthGrad) + 1e-5;
    return exp(-abs(targetDepth - thisDepth) / denom);
}
// 求一处像素的联合双边滤波深度权重项，减少重复采样的算法
float weightDepth(ivec2 thisIndex, ivec2 targetIndex, vec2 thisDepthGrad, comparison1D dep){
    // 系数是σD
    float denom = 100.0 * length(vec2(thisIndex) - vec2(targetIndex)) * length(thisDepthGrad) + 1e-5;
    return exp(-abs(dep.value0 - dep.value1) / denom);
}

// 求一处像素的联合双边滤波法线权重项
float weightNormal(vec3 thisNorm, vec3 targetNorm){
    // 指数是σN
    return pow(max(0, dot(thisNorm, targetNorm)), 100.0);
}

// 颜色的灰度
float Luminace(vec3 color){
    return color.r * 0.299 + color.g * 0.587 + color.b * 0.114;
}

// 求一处像素的联合双边滤波颜色权重项
float weightColor(float thisColorVar, comparison4D col){
    // 系数是σL
    float denom = 1.0 * sqrt(thisColorVar) + 0.1;
    return exp(-abs(Luminace(col.value0.xyz) + Luminace(col.value1.xyz)) / denom);
}

// image采样坐标过滤
ivec2 mirrorRepeat(ivec2 ori){
    ivec2 ans = ori;
    ans.x += screenWidth;
    ans.y += screenHeight;
    ans.x %= 2 * screenWidth;
    ans.y %= 2 * screenHeight;
    ans.x -= screenWidth;
    ans.y -= screenHeight;
    if (ans.x >= 0 && ans.x < screenWidth && ans.y >= 0 && ans.y < screenHeight){
        return ans;
    }
    else if (ans.x >= -screenWidth && ans.x < 0 && ans.y >= 0 && ans.y < screenHeight){
        return ivec2(ans.x + screenWidth, ans.y);
    }
    else if (ans.x >= 0 && ans.x < screenWidth && ans.y >= screenHeight && ans.y < 0){
        return ivec2(ans.x, ans.y + screenHeight);
    }
    else if (ans.x >= -screenWidth && ans.x < 0 && ans.y >= screenHeight && ans.y < 0){
        return ivec2(ans.x + screenWidth, ans.y + screenHeight);
    }
    return ans;
}
// 边缘clamp
ivec2 edgeClamp(ivec2 ori){
    return ivec2(clamp(ori.x, 0, screenWidth), clamp(ori.y, 0, screenHeight));
}

// 高斯
float gaussain(float x, float sigma){
    return exp(-x*x/(2*sigma*sigma))/(sqrt(2*PI)*sigma);
}

// 双边滤波
float bilateralFilterWeight(float x, float sigma, comparison4D color){
    float colorWeight = 1 / (length(vec3(color.value0.xyz - color.value1.xyz)) + 1);
    return gaussain(x, sigma) * colorWeight;
}

// 联合双边滤波
float jointBilateralFilterWeight(float x, float sigma, comparison4D color, comparison4D pos, comparison4D norm, comparison1D shadow){
    float colorWeight = 1 / (length(vec3(color.value0.xyz - color.value1.xyz)) + 1);
    float posWeight = 1 / pow(length(vec3(pos.value0.xyz - pos.value1.xyz)) + 1, 50);
    float normalWeight = exp(-500 * length(vec3(norm.value0.xyz - norm.value1.xyz)));
    float shadowWeight = exp(-3 * abs(shadow.value0 - shadow.value1));
    return gaussain(x, sigma) * min(0.1 * colorWeight + 0.0 * posWeight + 0.9 * normalWeight, shadowWeight);
}

void main(){
    // 首先清理无用kernel
    if (gl_GlobalInvocationID.x >= screenWidth || gl_GlobalInvocationID.y >= screenHeight){
        return;
    }
    if (filterStep < 0){
        return;
    }
    if (kernelRadius < 0){
        return;
    }
    ivec2 index = ivec2(gl_GlobalInvocationID.xy);
    vec4 srcColor = imageLoad(RTSrc, index);
    vec2 sampleTexCoord = vec2(vec2(gl_GlobalInvocationID.xy) + vec2(0.5, 0.5)) / vec2(screenWidth, screenHeight);
    vec4 srcWorldPos = texture(RTWorldPos, sampleTexCoord);
    vec4 srcWorldNormal = texture(RTNormal, sampleTexCoord);
    float srcDepth = texture(RTDepth, sampleTexCoord).r;
    float realDepth = LinearizeDepth(srcDepth);
    realDepth = LinearDepthValue(index);
    // float srcWorldDepth = length(cameraPos - srcWorldPos.xyz);
    
    // 优化低通滤波算法：a-trous wavelet
    // 由于同步问题，需要调用多次compute shader
    float interval = pow(2, filterStep);
    float WeightSum = 0;
    float sum = 0;
    vec4 valueColor = vec4(0);
    vec4 valueWorldPos = vec4(0);
    vec4 valueWorldNormal = vec4(0);
    vec4 ans = vec4(0);
    for(int line = -kernelRadius; line <= kernelRadius; line++){
        for(int column = -kernelRadius; column <= kernelRadius; column++){
            ivec2 thisIndex = edgeClamp(ivec2(index.x + interval * line, index.y + interval * column));
            // 采样间隔为interval位置的点，然后平均
            // 采样值
            valueColor = imageLoad(RTSrc, ivec2(thisIndex));

            vec2 thisSampleTexCoord = vec2(vec2(thisIndex.xy) + vec2(0.5, 0.5)) / vec2(screenWidth, screenHeight);
            valueWorldPos = texture(RTWorldPos, thisSampleTexCoord);
            valueWorldNormal = texture(RTNormal, thisSampleTexCoord);

            // 准备联合双边滤波的对比值
            comparison4D color;
            color.value0 = srcColor;
            color.value1 = valueColor;

            vec2 depGrad = LinearDepthGradience(index);
            comparison1D depComp;
            depComp.value0 = LinearDepthValue(index);
            depComp.value1 = LinearDepthValue(thisIndex);

            float thisColorVar = imageLoad(RTColorVariance, index).a;

            float Wd = weightDepth(index, thisIndex, depGrad, depComp);
            float Wn = weightNormal(srcWorldNormal.xyz, valueWorldNormal.xyz);
            float Wc = weightColor(thisColorVar, color);

            float w = Wd * 0.2 + Wn * 0.8;// + Wc * 0.3;
            // w = Wc;
            ans += valueColor * w;

            WeightSum += w;
        }
    }
    ans /= WeightSum;

    // ans = vec4(length(LinearDepthGradience(index)) .xxxx) * 20;// * 0.95 + ans * 0.05;

    imageStore(RTTarget, ivec2(gl_GlobalInvocationID.xy), ans);
}